
import java.io.IOException;
import java.util.*;
import java.lang.Math;

import org.apache.hadoop.fs.Path;
import org.apache.hadoop.conf.*;
import org.apache.hadoop.io.*;
import org.apache.hadoop.mapreduce.*;
//import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
//import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;

import java.util.Random;
import java.io.IOException;
import java.util.Iterator;
import java.util.StringTokenizer;

import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.FileInputFormat;
import org.apache.hadoop.mapred.FileOutputFormat;
import org.apache.hadoop.mapred.FileSplit;
import org.apache.hadoop.mapred.JobClient;
import org.apache.hadoop.mapred.JobConf;
import org.apache.hadoop.mapred.MapReduceBase;
import org.apache.hadoop.mapred.Mapper;
import org.apache.hadoop.mapred.OutputCollector;
import org.apache.hadoop.mapred.Reducer;
import org.apache.hadoop.mapred.Reporter;

import java.lang.Math;



public class new32p {


	

	
public static class Map extends MapReduceBase
implements Mapper<LongWritable, Text, Text, Text> { 
private   Text keym = new Text();
private 	Text value2 = new Text();



public void map(LongWritable key, Text value, OutputCollector<Text, Text> output, Reporter reporter) throws IOException {
	
	

	String input = value.toString();
	
StringTokenizer result =new StringTokenizer(input);
while (result.hasMoreTokens()) {

String word_temp2 = result.nextToken().toString();
	char[] split_word = word_temp2.toCharArray();
	

	
	String word_temp3 = word_temp2;
	word_temp3=word_temp3.substring(2);
	word_temp3= word_temp3.substring(0, word_temp3.length() - 1);
	StringTokenizer temp3 = new StringTokenizer(word_temp3, ",");
	 String[] split_word3 = new String[2];
	int ii=0;
	while (temp3.hasMoreTokens()) {
		String word_temp33=temp3.nextToken();
		split_word3[ii] = word_temp33;
			 ii+=1
					;
			 }

			
	String value1=new String("");
	String keyt=new String("");
	if (split_word[0]=='R')  { keyt=split_word3[1].toString();} else 
			if (split_word[0]=='S')  { keyt=split_word3[0];}
		if (split_word[0]=='R')  { value1=split_word3[0].toString();} else 
		if (split_word[0]=='S')  { value1=split_word3[1].toString();}
		char rel=split_word[0];

       String value2s=value1+","+rel+" ";
       String keyms=""+keyt;
     


       Text keym=new Text(keyms);
       Text value2=new Text(value2s);
       
		output.collect(keym, value2);		
	}


}
}
 

public static class Reduce extends MapReduceBase implements Reducer<Text, Text, Text, Text> {
public void reduce(Text key, Iterator<Text> values, OutputCollector<Text, Text> output, Reporter reporter) throws IOException {


	String input_redu = values.toString();
	
     StringBuilder toReturn = new StringBuilder(6000);
     while (values.hasNext()){

       toReturn.append(values.next().toString());
     }

  
	
	String  value_str = toReturn.toString();

StringTokenizer tokens =new StringTokenizer(value_str);




int lr=0;
int lt=0;
StringBuilder outp1 = new StringBuilder(6000);


while (tokens.hasMoreTokens()) {
String word_temp2r=tokens.nextToken();

	char[] split_word = word_temp2r.toCharArray();

	
	if (split_word[word_temp2r.length()-1]=='R')  {  lr+=1; } else 
			if (split_word[word_temp2r.length()-1]=='S')  { lt+=1;}


}
StringTokenizer tokens3 =new StringTokenizer(value_str);

while (tokens3.hasMoreTokens()) {
String word_temp2r=tokens3.nextToken();




	outp1=outp1.append(key+","+lr+","+lt+","+word_temp2r+" ");

}

String outp=outp1.toString();
Text outs=new Text(outp);
output.collect(key,outs);


}
}
public static class Map2 extends MapReduceBase
implements Mapper<LongWritable, Text, Text, Text> { 
private   Text keym2 = new Text();
private 	Text value22 = new Text();



public void map(LongWritable key, Text values, OutputCollector<Text, Text> output, Reporter reporter) throws IOException {
	
	String input_redu = values.toString();
	

StringTokenizer tokens =new StringTokenizer(input_redu);


		StringTokenizer tokens2 =new StringTokenizer(input_redu);

		int i=0;
		int j=0;
		String[] split_word =new String[5];

		String word_temp2r="";
		
while (tokens2.hasMoreTokens()) {
word_temp2r=tokens2.nextToken();
int wl=word_temp2r.length();
if (word_temp2r.charAt(wl-1)=='R' || word_temp2r.charAt(wl-1)=='S')
{


StringTokenizer tokens3 =new StringTokenizer(word_temp2r,",");
	split_word[0]=tokens3.nextToken();
	split_word[1]=tokens3.nextToken();
	split_word[2]=tokens3.nextToken();
	split_word[3]=tokens3.nextToken();
	split_word[4]=tokens3.nextToken();
	int lr=0;
	lr=Integer.parseInt(split_word[1]);
	int lt=0;
	lt=Integer.parseInt(split_word[2]);
	Random randomGenerator = new Random();
	
	int k=800000;
	int si=1;
	int ri=1;
	int si2=1;
	int ri2=1;
	si2 = ((int) Math.ceil(Math.sqrt(k/(lr+1))));
	  ri2= ((int) Math.ceil(Math.sqrt(k/(lt+1))));
	
if  ((si2>0)&&(ri2>0))
{
	if (lr*lt>2000000)
{
	si = si2;
	  ri= ri2;
		//System.out.printf("2: key: %s si,ri= %d, %d   lr,lr= %d, %d \n ",key,si,ri, lr, lt);

			
} 
else { 	//System.out.printf("0 ");

}
}
	

	int rand=randomGenerator.nextInt(ri);
	int secondkey=1;
	if (split_word[4].charAt(0)=='R') 
	{ 
		for (int randres=0; randres<ri; randres++)
		{
			if (rand==randres )
		{
				int secondkey11=secondkey;
			for (int splitresult=0; splitresult<si; splitresult++)
			{
				String key2i=""+split_word[0]+","+secondkey11;
		Text key2=new Text(key2i);
		String outp=""+word_temp2r+" ";
		Text outs=new Text(outp);
		output.collect(key2,outs);
		secondkey11+=ri;

			}
			i+=1;
		}	
		 secondkey+=1;


		}
	}

	else 
			if (split_word[4].charAt(0)=='S') 
			{
				int rand2=randomGenerator.nextInt(si);
				int secondkey2=1;
				
					for (int randres=0; randres<si; randres++)
					{
						
						if (rand2==randres )
					{					
							int secondkey22=secondkey2;


						for (int splitresult=0; splitresult<ri; splitresult++)
						{
							String key2i=""+split_word[0]+","+secondkey22;
					Text key2=new Text(key2i);
					String outp=""+word_temp2r+" ";
					Text outs=new Text(outp);
					output.collect(key2,outs);
					
					secondkey22+=1;

						}
						i+=1;
					}
						secondkey2+=ri;

					}
				}
	

   }
}
}
}

public static class Reduce2 extends MapReduceBase implements Reducer<Text, Text, Text, Text> {
public void reduce(Text key, Iterator<Text> values, OutputCollector<Text, Text> output, Reporter reporter) throws IOException {

	String input_redu = values.toString();

     StringBuilder toReturn = new StringBuilder(6000);
     while (values.hasNext()){

       toReturn.append(values.next().toString());
     }


     String  value_str = toReturn.toString();

     StringTokenizer tokens =new StringTokenizer(value_str);


 


	int lr=0;
	int lt=0;
	while (tokens.hasMoreTokens()) {
	String word_temp2r=tokens.nextToken();

		char[] split_word = word_temp2r.toCharArray();

		
		if (split_word[word_temp2r.length()-1]=='R')  {  lr+=1; } else 
				if (split_word[word_temp2r.length()-1]=='S')  { lt+=1;}
	}
	




		StringTokenizer tokens2 =new StringTokenizer(value_str);

		int i=0;
		int j=0;
		String[] split_word =new String[5];
		 String[] valuer;
		 String[] valuet;
		 valuer= new String[lr];
			valuet= new String[lt];

while (tokens2.hasMoreTokens()) {
String word_temp2r=tokens2.nextToken();


StringTokenizer tokens3 =new StringTokenizer(word_temp2r,",");
	
    split_word[0]=tokens3.nextToken();
	split_word[1]=tokens3.nextToken();
	split_word[2]=tokens3.nextToken();
	split_word[3]=tokens3.nextToken();
	split_word[4]=tokens3.nextToken();
	
	
	if (split_word[4].charAt(0)=='R')  { valuer[i]=split_word[3]; i+=1; 
} 
	else 
			if (split_word[4].charAt(0)=='S')  { valuet[j]=split_word[3]; j+=1; 
			}
	
}



StringTokenizer valuer2 =new StringTokenizer(valuer.toString(),",");
StringTokenizer valuet2 =new StringTokenizer(valuet.toString(),",");
String[] valuer3;
 String[] valuet3;
 if (lr>0) valuer3= new String[lr]; else valuer3= new String[1];
if (lt>0)	valuet3= new String[lt]; else valuet3= new String[1] ;
	i=0;
	while (valuer2.hasMoreTokens()) 
		{valuer3[i]=valuer2.nextToken();
		i+=1;
}
	i=0;
	while (valuet2.hasMoreTokens()) 
	{valuet3[i]=valuet2.nextToken();
	i+=1;
}

	



StringBuilder outp = new StringBuilder(6000);



for (int x=0; x<lr;  x++) {


	 for (int y=0; y<lt;  y++)
	{      
			outp=outp.append("T("+valuer[x]+","+split_word[0]+","+valuet[y]+")"+" ");


		
	}
}








   



String outp2=outp.toString();


key= new Text("key: "+key+" lr: "+lr+" ls: "+lt);

Text outs=new Text(outp2);
output.collect(key,outs);
}
}

public static void main(String[] args)  {

JobClient client = new JobClient();
JobConf conf = new JobConf(new32p.class);

conf.setJobName("WordCount");

//conf.setNumReduceTasks(2);



conf.setOutputKeyClass(Text.class);
conf.setOutputValueClass(Text.class);

FileInputFormat.addInputPath(conf, new Path("input"));
FileOutputFormat.setOutputPath(conf, new Path("output1"));

conf.setMapperClass(Map.class);
conf.setReducerClass(Reduce.class);


client.setConf(conf);

try {
	JobClient.runJob(conf);
} catch (Exception e) {
	e.printStackTrace();
}
////
JobConf conf2 = new JobConf(new32p.class);

conf2.setJobName("WordCount2");

//conf2.setNumReduceTasks(2);



conf2.setOutputKeyClass(Text.class);
conf2.setOutputValueClass(Text.class);

FileInputFormat.addInputPath(conf2, new Path("output1"));
FileOutputFormat.setOutputPath(conf2, new Path("output"));

conf2.setMapperClass(Map2.class);
conf2.setReducerClass(Reduce2.class);


client.setConf(conf2);


try {
	JobClient.runJob(conf2);
} catch (Exception e) {
	e.printStackTrace();
}
}
}


